{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9b1a11-78ba-4273-9573-cb70c1da2f03",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff7459-d101-4835-8a25-46dd87082ede",
   "metadata": {},
   "source": [
    "`NLTK` is an <u>open-source library</u> that simplifies the complexities of natural language processing (NLP). It offers functionalities for tasks like tokenization, stemming, tagging, parsing, and more. NLTK is widely used in academia and industry for building applications involving text analysis, sentiment analysis, language understanding, and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9e874-5a59-4abf-8f9b-a0918586f52a",
   "metadata": {},
   "source": [
    "## What is Text Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116520c5-52e3-4c1b-a588-b828178061a8",
   "metadata": {},
   "source": [
    "`Text processing` is the technique of transforming and analyzing textual data to make it easier to work with, especially for tasks like text mining, natural language processing (NLP), and machine learning. It involves cleaning, structuring, and preparing text so that it can be analyzed more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48c065-b435-41e8-8dfb-e3b48d657559",
   "metadata": {},
   "source": [
    "## Key steps in Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea3775-f908-4c26-84c9-bfc23d91c6b3",
   "metadata": {},
   "source": [
    "- **Tokenization**\n",
    "    - The breaking down of text into smaller units is called tokens. tokens are a small part of that text. If we have a sentence, the idea is to separate each word and build a vocabulary such that we can represent all words uniquely in a list. Numbers, words, etc.. all fall under tokens.\n",
    "      - ***Word Tokenization*** - is the process of splitting a sentence or text into individual words or tokens.\n",
    "      - ***Sentence Tokenization*** - (or segmentation) is the process of splitting text into individual sentences.\n",
    "    > Note: Basically its like using .split() function to split your text into a list.\n",
    "- **Word Casing**\n",
    "  - In text processing, while `lowercasing` is commonly used for standardizing text, there are situations where `uppercasing` or `title casing` (capitalizing the first letter of each word) might be required.\n",
    "- **Stop words**\n",
    "  - When presented features from a text to model, we might encounter a lot of noise. Removing common words like \"the,\" \"is,\" or \"and\" that do not contribute significant meaning to the analysis. With NLTK we can see all the stop words available in the English language.\n",
    "- **Contractions**\n",
    "  - Contractions are often expanded into their full forms to maintain uniformity and accuracy in text analysis. This step is critical since contractions like \"can't\" or \"won't\" are abbreviated forms of \"cannot\" and \"will not,\" and failing to expand them may result in errors in text processing.\n",
    "- **Stemming**\n",
    "  - Stemming is the process of reducing a word to its root or stem by chopping off its suffixes or prefixes. The result may not always be a valid word, but it serves as a rough approximation to group similar words together. How it works is that it uses simple heuristic rules to remove common word endings, such as \"-ing\", \"-ed\", \"-ly\", \"-s\", etc.\n",
    "  - uses simple heuristic rules to remove common word endings, such as \"-ing\", \"-ed\", \"-ly\", \"-s\", etc. It doesn’t take into account the meaning of the word or context, leading to quick and computationally efficient results.\n",
    "    * Types of Stemming:\n",
    "        - Porter Stemmer: One of the most commonly used stemming algorithms. It follows a set of rules to iteratively trim words down to their base form.\n",
    "        - Lancaster Stemmer: A more aggressive stemming algorithm that removes larger chunks from words but may lead to over-stemming (too much truncation).\n",
    "        - Snowball Stemmer: An improvement over Porter stemming, it’s also known as the English Stemmer and is more flexible in handling word forms.\n",
    "| Word | Porter | Lancaster |\n",
    "| ---- | ------ | ----------|\n",
    "|\"running\"|\t\"run\"|\t\"run\"|\n",
    "|\"runner\"|\t\"run\"|\t\"run\"|\n",
    "|\"happily\"|\t\"happili\"|\t\"happy\"|\n",
    "|\"studies\"|\t\"studi\"|\t\"study\"|\n",
    "|\"studying\"|\t\"studi\"|\t\"study\"|\n",
    "\n",
    "\n",
    "- **Lemmatization**\n",
    "  -  is the process of reducing a word to its lemma, or dictionary form, after evaluating its meaning and part of speech. Unlike stemming, lemmatization generates an appropriate word in the language. It uses more complex process, often relying on a vocabulary or lexical database (like WordNet) to look up words and return their canonical form based on their context, such as whether they are nouns, verbs, or adjectives.\n",
    "    * Types of Lemmatization:\n",
    "      - Rule-based Lemmatization: Relies on a set of rules to transform inflected words into their root form based on their part of speech.\n",
    "      - Dictionary-based Lemmatization: Uses a lookup table (like WordNet) to find the base form of words.\n",
    "|   Word\t|  Lemmatization (Verb)  |\tLemmatization (Noun)  |\n",
    "|-----------|------------------------|------------------------|\n",
    "|\"running\"  |\t\"run\"                |            \"run\"       |\n",
    "|\"better\"   |\t\"good\"               |         \"better\"       |\n",
    "|\"geese\"    |\t\"goose\"              |          \"goose\"       |\n",
    "|\"studies\"  |\t\"study\"              |          \"study\"       |\n",
    "|\"studying\" |\t\"study\"              |          \"study\"       |\n",
    "> Note: Lemmatization accurately returns meaningful words by considering context. For example, \"better\" as an adjective returns \"good,\" while as a verb it remains \"better.\"<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9785fe6b-98a5-46c9-a751-11fb68950728",
   "metadata": {},
   "source": [
    "### Addt'l Info: \n",
    "#### ***Regex*** or Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ec96d-222e-4d8e-9b29-5af52637bbd5",
   "metadata": {},
   "source": [
    "***Regular expressions*** (regex or regexp) are extremely useful in extracting information from any text by searching for one or more matches of a specific search pattern (i.e. a specific sequence of ASCII or unicode characters).\n",
    "\n",
    "***[Here is an in-depth understanding on RegEx](https://medium.com/@victoriousjvictor/understanding-regular-expressions-regex-e1c048f5aa6c)*** <br>\n",
    "***[Additional Information, ](https://archive.is/63sjK#selection-613.0-617.125)*** this is to go through some patterns and additional examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93c235-561f-403d-8311-74a3634fb04f",
   "metadata": {},
   "source": [
    "### Import necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "602fb0f8-1b52-47c1-86f3-ea45d36dbe7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:08.127622Z",
     "iopub.status.busy": "2025-08-28T06:20:08.127622Z",
     "iopub.status.idle": "2025-08-28T06:20:09.910580Z",
     "shell.execute_reply": "2025-08-28T06:20:09.910580Z",
     "shell.execute_reply.started": "2025-08-28T06:20:08.127622Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re # for Regex\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "984b1595-25a5-4980-b0e2-8135d370b19a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:15.083502Z",
     "iopub.status.busy": "2025-08-28T06:20:15.082507Z",
     "iopub.status.idle": "2025-08-28T06:20:18.246719Z",
     "shell.execute_reply": "2025-08-28T06:20:18.245715Z",
     "shell.execute_reply.started": "2025-08-28T06:20:15.083502Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/iragca/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/iragca/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/iragca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4c0e85-1164-4b7e-a260-db162854150a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:25.375202Z",
     "iopub.status.busy": "2025-08-28T06:20:25.375202Z",
     "iopub.status.idle": "2025-08-28T06:20:25.380877Z",
     "shell.execute_reply": "2025-08-28T06:20:25.379777Z",
     "shell.execute_reply.started": "2025-08-28T06:20:25.375202Z"
    }
   },
   "outputs": [],
   "source": [
    "myCorpus = [\" The quick brown fox wasn't that quick and he couldn't win the race\",\n",
    "         \"Hey! That's a grete deal! I just bought a /phone at $144 \",\n",
    "         \"@@You'll (learn) a **lot** in the book. Python fox is an amaewzing langage !@@\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e585d83-be3a-4408-a42f-fbb82b271b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:25.723860Z",
     "iopub.status.busy": "2025-08-28T06:20:25.722864Z",
     "iopub.status.idle": "2025-08-28T06:20:25.734857Z",
     "shell.execute_reply": "2025-08-28T06:20:25.733655Z",
     "shell.execute_reply.started": "2025-08-28T06:20:25.723860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf0ae2b-24ed-4db6-ac48-ddcb89c8a3fd",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74580a47-2415-4c5e-82d8-c94b8ea595e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:27.102353Z",
     "iopub.status.busy": "2025-08-28T06:20:27.101356Z",
     "iopub.status.idle": "2025-08-28T06:20:27.109644Z",
     "shell.execute_reply": "2025-08-28T06:20:27.108641Z",
     "shell.execute_reply.started": "2025-08-28T06:20:27.102353Z"
    }
   },
   "outputs": [],
   "source": [
    "def rem_punct(my_str):\n",
    "    \n",
    "    if my_str is None:\n",
    "        return None\n",
    "    \n",
    "    punctuations = '''!¡()-[]{};:'.'\"“”`\\,.<>/|?@#$%^—-=&_+0123456789•~*…''' \n",
    "\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct += char\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac4252a4-4128-421a-b43a-4e8cf66d0d86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:29.157782Z",
     "iopub.status.busy": "2025-08-28T06:20:29.157782Z",
     "iopub.status.idle": "2025-08-28T06:20:29.165291Z",
     "shell.execute_reply": "2025-08-28T06:20:29.164271Z",
     "shell.execute_reply.started": "2025-08-28T06:20:29.157782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The quick brown fox wasnt that quick and he couldnt win the race\n",
      "Hey Thats a grete deal I just bought a phone at  \n",
      "Youll learn a lot in the book Python fox is an amaewzing langage \n"
     ]
    }
   ],
   "source": [
    "for y in myCorpus:\n",
    "    rempunct = rem_punct(y)\n",
    "    print(rempunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c793b-7cb4-40e5-9280-ed6b986e3abf",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e11b35b1-753f-4481-961e-8fba1b3eabed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:30.832888Z",
     "iopub.status.busy": "2025-08-28T06:20:30.832888Z",
     "iopub.status.idle": "2025-08-28T06:20:30.840359Z",
     "shell.execute_reply": "2025-08-28T06:20:30.839358Z",
     "shell.execute_reply.started": "2025-08-28T06:20:30.832888Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_tokenize(text):\n",
    "    word_tokens = nltk.sent_tokenize(text)\n",
    "    return word_tokens\n",
    "\n",
    "def word_tokenize(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "248c530e-4357-4030-afc0-8a478bca2adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:32.717985Z",
     "iopub.status.busy": "2025-08-28T06:20:32.717985Z",
     "iopub.status.idle": "2025-08-28T06:20:32.898701Z",
     "shell.execute_reply": "2025-08-28T06:20:32.897692Z",
     "shell.execute_reply.started": "2025-08-28T06:20:32.717985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'wasnt', 'that', 'quick', 'and', 'he', 'couldnt', 'win', 'the', 'race']\n",
      "['Hey', 'Thats', 'a', 'grete', 'deal', 'I', 'just', 'bought', 'a', 'phone', 'at']\n",
      "['Youll', 'learn', 'a', 'lot', 'in', 'the', 'book', 'Python', 'fox', 'is', 'an', 'amaewzing', 'langage']\n"
     ]
    }
   ],
   "source": [
    "new_list = [rem_punct(sentence) for sentence in myCorpus]\n",
    "for word in new_list:\n",
    "    token = word_tokenize(word)\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47226667-6ce8-4f30-8d29-73d16fed9c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:36.343325Z",
     "iopub.status.busy": "2025-08-28T06:20:36.343325Z",
     "iopub.status.idle": "2025-08-28T06:20:36.350357Z",
     "shell.execute_reply": "2025-08-28T06:20:36.349355Z",
     "shell.execute_reply.started": "2025-08-28T06:20:36.343325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "I found a new hobby to do.\n",
      "How is life?\n"
     ]
    }
   ],
   "source": [
    "sentence_corpus = 'Hello World! I found a new hobby to do. How is life?'\n",
    "\n",
    "token_sent = sent_tokenize(sentence_corpus)\n",
    "for sentence in token_sent:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5198d8-5a67-4b91-8ce1-4c8053e67db8",
   "metadata": {},
   "source": [
    "### Removing Repeating Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d56435e-b190-493f-8dc0-4f8929156fe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:38.650913Z",
     "iopub.status.busy": "2025-08-28T06:20:38.650913Z",
     "iopub.status.idle": "2025-08-28T06:20:38.657430Z",
     "shell.execute_reply": "2025-08-28T06:20:38.656421Z",
     "shell.execute_reply.started": "2025-08-28T06:20:38.650913Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def removeRepeatedCharacters(tokens):\n",
    "    repeatPattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    \n",
    "    def replace(word):\n",
    "        while not wordnet.synsets(word):\n",
    "            newWord = repeatPattern.sub(r'\\1\\2\\3', word)\n",
    "            if newWord == word:\n",
    "                break\n",
    "            word = newWord\n",
    "        return word\n",
    "\n",
    "    return [replace(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d58db-b2aa-4b29-8750-8b016dd71b7d",
   "metadata": {},
   "source": [
    "> ***Wild Regex pattern appeared!*** <br>\n",
    "> r'(\\w*)(\\w)\\2(\\w*)' This pattern looks for repeated characters. The \\2 refers to the second group, meaning it captures any character repeated consecutively.\n",
    "\n",
    ">r'\\1\\2\\3': This pattern replaces the repeated characters with a single occurrence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28fe8729-64ba-47e0-a4e7-934d6c3ac32d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:41.801288Z",
     "iopub.status.busy": "2025-08-28T06:20:41.800290Z",
     "iopub.status.idle": "2025-08-28T06:20:44.993390Z",
     "shell.execute_reply": "2025-08-28T06:20:44.993390Z",
     "shell.execute_reply.started": "2025-08-28T06:20:41.801288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence  ['My', 'Schoooooool', 'is', 'reeeeeeaaaallllllllly', 'amaaaaaazingggg', '!']\n",
      "Corrected Sentence  ['My', 'School', 'is', 'realy', 'amazing', '!']\n"
     ]
    }
   ],
   "source": [
    "sentence = ' My Schoooooool is reeeeeeaaaallllllllly amaaaaaazingggg!'\n",
    "\n",
    "sampleSentence = word_tokenize(sentence)\n",
    "print(\"Original Sentence \", sampleSentence)\n",
    "print(\"Corrected Sentence \", removeRepeatedCharacters(sampleSentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa780f-8740-42d2-9ba4-9ad845234646",
   "metadata": {},
   "source": [
    "### Word Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91917146-11c6-4746-a3e0-532531ada5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:50.157437Z",
     "iopub.status.busy": "2025-08-28T06:20:50.157437Z",
     "iopub.status.idle": "2025-08-28T06:20:50.218388Z",
     "shell.execute_reply": "2025-08-28T06:20:50.216221Z",
     "shell.execute_reply.started": "2025-08-28T06:20:50.157437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'was', 'not', 'that', 'quick', 'and', 'he', 'could', 'not', 'win', 'the', 'race']\n",
      "['Hey', 'That', 'Is', 'a', 'grete', 'deal', 'I', 'just', 'bought', 'a', 'phone', 'at']\n",
      "['You', 'Will', 'learn', 'a', 'lot', 'in', 'the', 'book', 'Python', 'fox', 'is', 'an', 'amaewzing', 'langage']\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "for word in new_list:\n",
    "    contra = contractions.fix(word)\n",
    "    token = word_tokenize(contra)\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f6b397-b628-4a6a-836f-76e044af6a8c",
   "metadata": {},
   "source": [
    "### Remove Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2b8b68-ddc8-4478-a1af-883288552883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:52.750831Z",
     "iopub.status.busy": "2025-08-28T06:20:52.749834Z",
     "iopub.status.idle": "2025-08-28T06:20:52.756605Z",
     "shell.execute_reply": "2025-08-28T06:20:52.756092Z",
     "shell.execute_reply.started": "2025-08-28T06:20:52.750831Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords_orig(text):\n",
    "    global filtered_words\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered_words = []\n",
    "    for w in text:\n",
    "        if w not in stopWords:\n",
    "            filtered_words.append(w)\n",
    "            filtered_words.append(' ')\n",
    "           \n",
    "    return \"\".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c30dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2977cb56-ba0c-49de-8f66-ad62b981b535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:54.425015Z",
     "iopub.status.busy": "2025-08-28T06:20:54.425015Z",
     "iopub.status.idle": "2025-08-28T06:20:54.477057Z",
     "shell.execute_reply": "2025-08-28T06:20:54.475975Z",
     "shell.execute_reply.started": "2025-08-28T06:20:54.425015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick brown fox quick could win race \n",
      "hey grete deal bought phone \n",
      "learn lot book python fox amaewzing langage \n"
     ]
    }
   ],
   "source": [
    "for word in new_list:\n",
    "    word = word.lower()\n",
    "    contra = contractions.fix(word)\n",
    "    token = word_tokenize(contra)\n",
    "    no_stopwords = remove_stopwords_orig(token)\n",
    "    print(no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d285b-d8d4-46d7-87b3-10ba903ef5ec",
   "metadata": {},
   "source": [
    "> *Note*: <br>The stopwords list provided by nltk (or similar libraries) contains only lowercase versions of words. This is typical because stopwords are usually considered case-insensitive. If your input text has capitalized words (like \"Is\" or \"I\"), they will not match with the lowercase stopwords unless you normalize the case by converting all words to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166621a-40a3-4f52-85e4-1299354ea59e",
   "metadata": {},
   "source": [
    "### Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1a0c817-2bf8-48c2-9796-0092291ce039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:20:57.081287Z",
     "iopub.status.busy": "2025-08-28T06:20:57.080229Z",
     "iopub.status.idle": "2025-08-28T06:20:57.093380Z",
     "shell.execute_reply": "2025-08-28T06:20:57.092358Z",
     "shell.execute_reply.started": "2025-08-28T06:20:57.080229Z"
    }
   },
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "def check_spelling(text):\n",
    "    spellcheck = Speller(lang='en')\n",
    "    return spellcheck(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42e8aabc-7780-4c1f-b9a5-b094957a5735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:21:00.466115Z",
     "iopub.status.busy": "2025-08-28T06:21:00.465119Z",
     "iopub.status.idle": "2025-08-28T06:21:00.814505Z",
     "shell.execute_reply": "2025-08-28T06:21:00.813498Z",
     "shell.execute_reply.started": "2025-08-28T06:21:00.465119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick brown fox quick could win race \n",
      "hey greet deal bought phone \n",
      "learn lot book python fox amazing language \n"
     ]
    }
   ],
   "source": [
    "for word in new_list:\n",
    "    word = word.lower()\n",
    "    contra = contractions.fix(word)\n",
    "    token = word_tokenize(contra)\n",
    "    no_stopwords = remove_stopwords_orig(token)\n",
    "    corr_spell = check_spelling(no_stopwords)\n",
    "    print(corr_spell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346853c-09a5-4962-8c28-a9fa20322f24",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36b8f4a2-6454-446e-8ed4-bf6b0424f11e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:21:02.324748Z",
     "iopub.status.busy": "2025-08-28T06:21:02.324748Z",
     "iopub.status.idle": "2025-08-28T06:21:02.334479Z",
     "shell.execute_reply": "2025-08-28T06:21:02.332448Z",
     "shell.execute_reply.started": "2025-08-28T06:21:02.324748Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "port_stem = PorterStemmer()\n",
    "lancas_stem = LancasterStemmer()\n",
    "snowb_stem = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40ca6a5e-a43b-4aea-b3cc-0505fa9a14fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:21:04.082443Z",
     "iopub.status.busy": "2025-08-28T06:21:04.081442Z",
     "iopub.status.idle": "2025-08-28T06:21:04.103212Z",
     "shell.execute_reply": "2025-08-28T06:21:04.102157Z",
     "shell.execute_reply.started": "2025-08-28T06:21:04.082443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'studi'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'study'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'studi'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'costli'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cost'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cost'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(port_stem.stem('studying'),\n",
    "       lancas_stem.stem('studying'),\n",
    "       snowb_stem.stem('studying'))\n",
    "print('----'*20)\n",
    "display(port_stem.stem('running'),\n",
    "       lancas_stem.stem('running'),\n",
    "       snowb_stem.stem('running'))\n",
    "print('----'*20)\n",
    "display(port_stem.stem('costly'),\n",
    "       lancas_stem.stem('costly'),\n",
    "       snowb_stem.stem('costly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee382ae-3c40-4379-ba02-97f22a12a512",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "091da0e3-27d7-4b28-b804-da585d6685bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:21:06.427265Z",
     "iopub.status.busy": "2025-08-28T06:21:06.426244Z",
     "iopub.status.idle": "2025-08-28T06:21:06.433349Z",
     "shell.execute_reply": "2025-08-28T06:21:06.431803Z",
     "shell.execute_reply.started": "2025-08-28T06:21:06.427265Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wlm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9274700c-dcff-4829-927e-f30ea4b31cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:21:07.766481Z",
     "iopub.status.busy": "2025-08-28T06:21:07.766481Z",
     "iopub.status.idle": "2025-08-28T06:21:07.778954Z",
     "shell.execute_reply": "2025-08-28T06:21:07.778449Z",
     "shell.execute_reply.started": "2025-08-28T06:21:07.766481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'understand'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'child'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(wlm.lemmatize('understanding', 'v'),\n",
    "        wlm.lemmatize('running', 'v'),\n",
    "        wlm.lemmatize('happiest', 'a'),\n",
    "        wlm.lemmatize('children', 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff168a-ed4d-42db-ad33-9fb616bc2f70",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d76bf4bb-ae1e-4cc2-b9cd-ffe4a24223c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:21:09.390030Z",
     "iopub.status.busy": "2025-08-28T06:21:09.390030Z",
     "iopub.status.idle": "2025-08-28T06:21:09.734310Z",
     "shell.execute_reply": "2025-08-28T06:21:09.733292Z",
     "shell.execute_reply.started": "2025-08-28T06:21:09.390030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick\n",
      "brown\n",
      "fox\n",
      "quick\n",
      "could\n",
      "win\n",
      "race\n",
      "hey\n",
      "greet\n",
      "deal\n",
      "buy\n",
      "phone\n",
      "learn\n",
      "lot\n",
      "book\n",
      "python\n",
      "fox\n",
      "amaze\n",
      "language\n"
     ]
    }
   ],
   "source": [
    "corr_list = []\n",
    "for word in new_list:\n",
    "    word = word.lower()\n",
    "    contra = contractions.fix(word)\n",
    "    token = word_tokenize(contra)\n",
    "    no_stopwords = remove_stopwords_orig(token)\n",
    "    corr_spell = check_spelling(no_stopwords)\n",
    "    corr_list.append(corr_spell)\n",
    "    \n",
    "for sentence in corr_list:\n",
    "    token = word_tokenize(sentence)\n",
    "    for word in token:\n",
    "        word = wlm.lemmatize(word, 'v')\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919557a8-4b96-434f-a43a-74ea05dd8900",
   "metadata": {},
   "source": [
    "As you can see, removing punctuations, expanding contractions, and spell checking are included here without the need for the NLTK Python package. These tasks help de-clutter our textual data, making it cleaner for more accurate analysis. These functions are part of what we call ***Data Cleaning***!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620cfd0-be65-4994-87f4-3fe46b20c996",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f45fb6-537a-48d4-afad-5412fbc0a394",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Data Normalization in Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8f993-0f39-480e-961b-b4404b22d47b",
   "metadata": {},
   "source": [
    "<font size=4px><b><i>Data Preprocessing</i></b></font>\n",
    "- for text involves a series of steps to clean, format, and transform raw text into a structured and analyzable form. It is the first stage before applying any machine learning or natural language processing (NLP) models.\n",
    "> <b>Goal</b>: To reduce noise and prepare the text for further analysis by ensuring it's structured, clean, and ready for modeling.\n",
    "\n",
    "\n",
    "\n",
    "<font size=4px><b><i>Data Normalization</i></b></font>\n",
    "- refers to techniques used to standardize and format the text consistently. This ensures that variations in word forms, cases, and other features don't confuse text analysis models.\n",
    "> <b>Goal</b>: To standardize and simplify the text by reducing variations and inconsistencies so that machine learning models can interpret it effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfada97-368f-4aae-b9e9-c6d7f0765bb4",
   "metadata": {},
   "source": [
    "In the context of Text Data, `Data Preprocessing` and `Data Normalization` are not *separate* stages, but rather integrated components of the same workflow that help us prepare unstructured text for deeper analysis.\n",
    "\n",
    "Consider Data Preprocessing to be a comprehensive procedure that includes tasks such as cleaning, organizing, and altering raw text. Tokenization, stopword removal, and dealing with noisy data like punctuation and special characters are all difficulties that must be addressed during preprocessing. These are the core tasks that will ensure that our material is manageable for analysis.\n",
    "\n",
    "Within this larger process, Data Normalization serves as a more targeted stage. Normalization ensures text consistency by standardizing diverse representations of the same information. \n",
    "\n",
    "Normalization, whether by converting everything to lowercase, extending contractions, or consistently formatting numbers and dates, helps us reduce variability and ensure that our research is not thrown off by little, inconsequential variances in the data.\n",
    "\n",
    "For example, in a preprocessing work, we might have tokenized a sentence and deleted stopwords. Then, normalization assures that changes in word forms (for example, 'USA' vs. 'usa') do not result in inaccurate textual insights.\n",
    "\n",
    "Thus, Data Normalization is an important subset of Data Preprocessing, especially when dealing with textual data, and they work together to lay the groundwork for any text-based data science tasks. Without proper preprocessing and normalization, our models may struggle to derive useful insights from the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257e123-9afc-4b7b-a467-2024b83aa26f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750d003-5038-4298-bd98-f0830a87aa3d",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b1db14b-eed7-4d81-bec2-1235d5c18934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:21:34.366311Z",
     "iopub.status.busy": "2025-08-28T06:21:34.364917Z",
     "iopub.status.idle": "2025-08-28T06:21:35.487986Z",
     "shell.execute_reply": "2025-08-28T06:21:35.487117Z",
     "shell.execute_reply.started": "2025-08-28T06:21:34.366311Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import pdfplumber\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ee580-fda7-4674-84c1-f2a35a9f083d",
   "metadata": {},
   "source": [
    "### Json Movie Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a57b451-1af4-4fa7-a488-7ba990acde2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T06:21:37.524069Z",
     "iopub.status.busy": "2025-08-28T06:21:37.524069Z",
     "iopub.status.idle": "2025-08-28T06:21:41.260669Z",
     "shell.execute_reply": "2025-08-28T06:21:41.260669Z",
     "shell.execute_reply.started": "2025-08-28T06:21:37.524069Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Datasets/movies/'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m path = \u001b[33m'\u001b[39m\u001b[33m./Datasets/movies/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m movies = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m movie_list = \u001b[38;5;28mlist\u001b[39m()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m years \u001b[38;5;129;01min\u001b[39;00m tqdm(movies):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './Datasets/movies/'"
     ]
    }
   ],
   "source": [
    "path = './Datasets/movies/'\n",
    "movies = os.listdir(path)\n",
    "movie_list = list()\n",
    "\n",
    "for years in tqdm(movies):\n",
    "    for movie in os.listdir(os.path.join(f'{path}', f'{years}')):\n",
    "        with open(os.path.join(f'{path}', f'{years}', f'{movie}'), encoding=\"ISO-8859-1\") as movie_jsn:\n",
    "            movie_list.append(json.load(movie_jsn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411a431-7583-4392-98d7-f08041611e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_df = pd.DataFrame(movie_list)\n",
    "movie_df = movie_df.applymap(lambda x: None if isinstance(x, str) and x.strip() == '' else x)\n",
    "# movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e165c-a0e0-4d3e-afef-5baa52322cb5",
   "metadata": {},
   "source": [
    "## Data Normalization (Sample For DataFrames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350f504-00a0-4551-bc22-8dbade30094a",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ae707-9693-465c-9edd-12e61cbe551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986d1b5-af61-403b-bbef-6d48ce1a6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- for integers and float ----- #\n",
    "for cols in ['year', 'runtime']:\n",
    "    movie_df[cols] = pd.to_numeric(movie_df[cols], errors='coerce')\n",
    "# or you can use df[column].astype('int64') or df[column].astype('float64')\n",
    "\n",
    "# ----- for strings ----- #\n",
    "movie_df[['name','categories', 'director', \n",
    "          'writer', 'actors', 'storyline']] = movie_df[['name','categories', 'director', \n",
    "                                                        'writer', 'actors', 'storyline']].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6daf7-1053-42cf-b796-31e1971efcff",
   "metadata": {},
   "source": [
    "### Word Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61247ed-f59c-4015-bb79-b89da9fc74bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_df['name'] = movie_df['name'].str.upper()\n",
    "movie_df\n",
    "# or you can do: movie_df['name'] = movie_df['name'].str.lower()\n",
    "# or: movie_df['name'] = movie_df['name'].str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2f471-7e0b-4d31-b6d1-215b8a353033",
   "metadata": {},
   "source": [
    "### Date Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c069dcf-be50-404e-bf63-a80e2b9e82fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -------- Nomralize the data's date to YYYY-MM-DD -------- #\n",
    "movie_df['release-date'] = pd.to_datetime(movie_df['release-date'], format='%Y-%m-%d', errors='coerce')\n",
    "display(movie_df, movie_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbecffcd-53f4-4fae-8393-3321a24374df",
   "metadata": {},
   "source": [
    "## Data Preprocessing (Sample For DataFrames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef41b8-9347-46da-8799-cc32eb3c5d77",
   "metadata": {},
   "source": [
    "### Finding Null Values\n",
    "<font size='2px'>At this point, we are only focusing on the <u>name</u> column up to the <u>storyline</u> column</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc60b34-fe27-4305-9b69-ee4d3fb18013",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2447b5-3ca4-4f18-a7ee-9e12dbe87359",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df[movie_df['runtime'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bcbcf3-bbb9-4fa5-941a-4c1cb1fa0f30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_df[movie_df['release-date'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ad038-91c3-4700-82ad-0e7ab3887e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this case, there is a value 'nan'\n",
    "movie_df[movie_df['categories'] == 'nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3fcb70-a765-48c6-bd06-98ec84e73a0f",
   "metadata": {},
   "source": [
    "#### Optional: Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd096b-fac1-47bf-a152-dbd3417cedf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = movie_df[['description', 'future', 'plot', 'released', 'genre', 'category', 'directors']]\n",
    "for x in sample.columns:\n",
    "    print(f'Column : {x}')\n",
    "    display(movie_df[movie_df[x].notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37139cd2-137c-40fb-a7d1-387ad21b0cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.drop(columns=['description', 'future', 'plot','released', 'genre', 'category', 'directors'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9f646-527e-481d-99f9-dac166b6a369",
   "metadata": {},
   "source": [
    "### Remove Punctuations, Numbers, and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24161b9-7dea-41d4-b152-e86a48939e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punct(my_str):\n",
    "    if my_str is None:\n",
    "        return None\n",
    "    \n",
    "    punctuations = '''!¡()-[]{};:'.'\"“”`\\,.<>/|?@#$%^—-=&_+0123456789•~*…''' \n",
    "\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct += char\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d44cfc-9da7-4e6a-a9dc-4f5318a8aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df['categories'] = movie_df['categories'].apply(rem_punct)\n",
    "movie_df['categories'] = movie_df['categories'].apply(lambda row: \", \".join(row.split(' ')) if isinstance(row, str) else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2b3ac-dd50-4856-9fa1-3be49e541217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6bde8b-309e-46ae-8f4e-1210f4788b74",
   "metadata": {},
   "source": [
    "### Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f1c86-df16-4878-aac3-dbf992336a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latin_char(name):\n",
    "    if isinstance(name, str):\n",
    "        normalized_text = unicodedata.normalize('NFKD', name)\n",
    "        encoded_text = normalized_text.encode('ASCII', 'ignore')\n",
    "        cleaned_name = encoded_text.decode('utf-8')\n",
    "        return cleaned_name\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec43aa-ab58-4cda-aee1-04de29d11bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df['name'] = movie_df.name.apply(latin_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ff07f-e1de-4173-89f1-eea84253dd78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_df.iloc[426]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e216f-38b5-4c14-be90-32dd6f3b70ae",
   "metadata": {},
   "source": [
    "### Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c41ef-e604-4d41-a3c5-d849da8680d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dup_movies = movie_df[movie_df.duplicated(subset=['name'], keep=False)].sort_values('name')\n",
    "dup_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8879a1-ba41-4260-8a8c-79b3ee71a6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dup_movies_year = movie_df[movie_df.duplicated(subset=['name', 'year'], keep=False)]#.sort_values('name')\n",
    "dup_movies_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db41fd-297d-4426-b364-d2ce010d2916",
   "metadata": {},
   "source": [
    "`dup_movies_year` has some discrepencies, having the same movie title and released at the same year but their release date are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770eaab-aae2-4d78-a62f-52973b7b8540",
   "metadata": {},
   "source": [
    "#### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fce17e-69c7-4a78-be54-b79585bc77c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_df_new = movie_df.drop_duplicates(subset=['name', 'year'], keep='last')\n",
    "movie_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f190f5e-ef7b-4eed-83be-2c749faafc97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------- or you can use the parameter 'inplace' in drop_duplicates() ------- #\n",
    "# ---------------- to overwrite directly the main dataframe ----------------- #\n",
    "\n",
    "movie_df.drop_duplicates(subset=['name', 'year'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8de6b3-4144-46c6-b987-cd475031af40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35eadc3-6b77-4fcf-8eb0-0b77d4d429ce",
   "metadata": {},
   "source": [
    "## Another Example!\n",
    "<font size='3px'><b>Let's test this on a pdf dataset</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0591d23-d239-4ea6-9db5-1001384c5bec",
   "metadata": {},
   "source": [
    "### PDF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02442b1f-d7d0-4c0b-9997-b28a98fb1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(\"./Datasets/sampledata_PDF.pdf\") as pdf:\n",
    "    tb = []\n",
    "    for i in tqdm(range(len(pdf.pages))):\n",
    "        page = ' '.join(pdf.pages[i].extract_text().split('\\n')[1:-1])\n",
    "        tb.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8db318-5a80-4ce5-860a-c47862c8549f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf_ = [text for text in tb if text]\n",
    "pdf_ = str(pdf_)[1:-1]\n",
    "pdf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743eb52e-6f91-42f8-92bd-beb2fc706697",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef572368-ee91-40a6-86fc-58cd26721730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punct(my_str):\n",
    "    \n",
    "    if my_str is None:\n",
    "        return None\n",
    "    \n",
    "    punctuations = '''!¡()-[]{};:''\"“”`\\,<>/|?@#$%^—-=&_+0123456789•~*…''' \n",
    "\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct += char\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc18c3-b931-4ab8-ba80-c82ffef0c39b",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7bfc7f-c227-4917-8919-bc6bebaaae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(text):\n",
    "    word_tokens = nltk.sent_tokenize(text)\n",
    "    return word_tokens\n",
    "\n",
    "def word_tokenize(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88defc8-5027-4d8d-b8fc-1cd4506e3e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf_ = rem_punct(pdf_)\n",
    "pdf_sent = sent_tokenize(pdf_)[0:3]\n",
    "pdf_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35047b4-682e-4b9b-a9fa-c1a9aedf118e",
   "metadata": {},
   "source": [
    "### Word Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bff55f-34c3-46c4-bacc-58b862ceb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "pdf_contra=[]\n",
    "for sent in pdf_sent:\n",
    "    rempunct = rem_punct(sent)\n",
    "    contra = contractions.fix(rempunct)\n",
    "    token = word_tokenize(contra)[:-1]\n",
    "    pdf_contra.append(' '.join(token))\n",
    "    \n",
    "pdf_contra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d7381-2409-4cf5-ab3b-7c1191b1d4b0",
   "metadata": {},
   "source": [
    "### Remove Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e3dba-beac-4b77-945b-504561bb4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords_orig(text):\n",
    "    global filtered_words\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered_words = []\n",
    "    for w in text:\n",
    "        if w not in stopWords:\n",
    "            filtered_words.append(w)\n",
    "            filtered_words.append(' ')\n",
    "           \n",
    "    return \"\".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee786d6-16dc-438b-995c-a5bb29f66f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_stop = []\n",
    "\n",
    "for word in pdf_contra:\n",
    "    word = word.lower()\n",
    "    token = word_tokenize(word)\n",
    "    no_stopwords = remove_stopwords_orig(token)\n",
    "    pdf_stop.append(no_stopwords.strip())\n",
    "\n",
    "pdf_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d6d44-e12d-4b9e-9f3f-5a0c0dd7a4a0",
   "metadata": {},
   "source": [
    "### Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5bd61-9640-44f1-a009-9cba10ce5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "def check_spelling(text):\n",
    "    spellcheck = Speller(lang='en')\n",
    "    return spellcheck(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0423dd-7fef-48db-860e-d11b476006b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_spell = []\n",
    "\n",
    "for word in pdf_stop:\n",
    "    # token = word_tokenize(word)\n",
    "    corr_spell = check_spelling(word)\n",
    "    pdf_spell.append(corr_spell)\n",
    "pdf_spell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ad2e1-203c-48be-9723-32633f783e8c",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1efa61-4047-416f-be58-02b055d9abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wlm = WordNetLemmatizer()\n",
    "snowb_stem = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b8417-5a34-4f64-9d3c-22ef12990699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf_final = []\n",
    "\n",
    "for sentence in pdf_spell:\n",
    "    sentence = word_tokenize(sentence)\n",
    "    pdf_final.append(sentence)\n",
    "\n",
    "pdf_final_stem = [snowb_stem.stem(word) for sentence in pdf_final for word in sentence]\n",
    "pdf_final_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2f99c-5fde-45e1-a1cd-8c2f4711564f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf_final_lemm = [wlm.lemmatize(word, 'v') for sentence in pdf_final for word in sentence]\n",
    "pdf_final_lemm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
